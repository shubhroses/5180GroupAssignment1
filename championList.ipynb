{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import os\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "indToDoc = {}\n",
    "postingList = collections.defaultdict(list)\n",
    "curDocID = 0\n",
    "path = \"practice3_collection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(f, path):\n",
    "    #Returns [(term, pos), (term, pos) ...]\n",
    "    terms = []\n",
    "    with open(path + \"/\" + f) as file:\n",
    "        line = file.readline()\n",
    "        i = 0\n",
    "        while line:\n",
    "            # Regex to match only strings and spaces \n",
    "            line = re.sub(r'[^A-Za-z\\s]+', '', line)\n",
    "            for word in line.split():\n",
    "                terms.append((word.lower(), i))\n",
    "                i += 1\n",
    "            line = file.readline() \n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPostingList(path):\n",
    "    startTime = time.time()\n",
    "    for f in os.listdir(path):\n",
    "        # TODO: remove in real code\n",
    "        global curDocID\n",
    "        \n",
    "        indToDoc[curDocID] = f\n",
    "\n",
    "        # Returns [(word, pos), (word, pos) ...]\n",
    "        terms = tokenize(f, path)\n",
    "\n",
    "        # create map {word:[pos1, pos2, pos3]}\n",
    "        wordToPos = collections.defaultdict(list)\n",
    "        for word, pos in terms:\n",
    "            wordToPos[word].append(pos)\n",
    "\n",
    "        # append to posting list {term : [(docID1, [pos1, pos2, pos3, pos4])]}\n",
    "        for term, arr in wordToPos.items():\n",
    "            postingList[term].append((curDocID, wordToPos[term]))\n",
    "\n",
    "        # For every file update id \n",
    "        curDocID += 1\n",
    "    \n",
    "    endTime = time.time()\n",
    "    print(f\"Index built in {endTime - startTime} seconds.\")\n",
    "    return postingList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index built in 0.0018129348754882812 seconds.\n"
     ]
    }
   ],
   "source": [
    "result = createPostingList(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPostingList = collections.defaultdict(list)\n",
    "idToTerm = {}\n",
    "curTermIndex = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'on', 'and', 'as', 'from', 'in', 'a', 'with', 'will', 'is', 'were', 'an', 'at', 'are', 'he', 'by', 'it', 'its', 'of', 'has', 'that', 'be', 'was', 'to', 'for', 'the'}\n"
     ]
    }
   ],
   "source": [
    "stopList = set()\n",
    "stopTuples = tokenize(\"stop-list.txt\", \".\")\n",
    "for t, p in stopTuples:\n",
    "    stopList.add(t)\n",
    "print(stopList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in result.items():\n",
    "    # if k in stopList:\n",
    "    #     continue\n",
    "    idToTerm[curTermIndex] = k\n",
    "    newPostingList[curTermIndex].append(math.log10(len(indToDoc)/len(v)))\n",
    "    \n",
    "    for docId, posList in v:\n",
    "        newPostingList[curTermIndex].append((docId, 1 + math.log10(len(posList)), posList))\n",
    "    curTermIndex += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : [0.17609125905568124, (0, 1.3010299956639813, [0, 4]), (1, 1.3010299956639813, [0, 4])]\n",
      "1 : [0.17609125905568124, (0, 1.3010299956639813, [1, 6]), (1, 1.0, [1])]\n",
      "2 : [0.47712125471966244, (0, 1.0, [2])]\n",
      "3 : [0.17609125905568124, (0, 1.0, [3]), (1, 1.0, [3])]\n",
      "4 : [0.47712125471966244, (0, 1.0, [5])]\n",
      "5 : [0.17609125905568124, (1, 1.0, [2]), (2, 1.0, [2])]\n",
      "6 : [0.17609125905568124, (1, 1.0, [5]), (2, 1.0, [1])]\n",
      "7 : [0.47712125471966244, (2, 1.0, [0])]\n"
     ]
    }
   ],
   "source": [
    "# CHAMP\n",
    "championsList = collections.defaultdict(list)\n",
    "r = 2\n",
    "# For each term, compute the r docs of highest weight in t's postrings\n",
    "for k, v in newPostingList.items():\n",
    "    championsList[k].append(v[0])\n",
    "    docs = v[1:]\n",
    "    docs.sort(key = lambda x: x[1], reverse = True)\n",
    "    topR = docs[:r]\n",
    "    championsList[k].extend(topR)\n",
    "\n",
    "for k, v in championsList.items():\n",
    "    print(f\"{k} : {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2}\n"
     ]
    }
   ],
   "source": [
    "# CHAMP\n",
    "champDocs = set()\n",
    "for k, v in championsList.items():\n",
    "    docs = v[1:]\n",
    "    justDocs = [d[0] for d in docs]\n",
    "    champDocs = champDocs.union(set(justDocs))\n",
    "print(champDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHAMP\n",
    "docToVecChamp = {}\n",
    "for k in champDocs:\n",
    "    docToVecChamp[k] = len(idToTerm)*[float(0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHAMP\n",
    "for k, v in championsList.items():\n",
    "    idf = v[0]\n",
    "    for i in range(1, len(v)):\n",
    "        t = v[i]\n",
    "        doc, w = t[0], t[1]\n",
    "        docToVecChamp[doc][k] = idf*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d1.txt : [0.22910001000567795, 0.22910001000567795, 0.47712125471966244, 0.17609125905568124, 0.47712125471966244, 0.0, 0.0, 0.0]\n",
      "d2.txt : [0.22910001000567795, 0.17609125905568124, 0.0, 0.17609125905568124, 0.0, 0.17609125905568124, 0.17609125905568124, 0.0]\n",
      "d3.txt : [0.0, 0.0, 0.0, 0.0, 0.0, 0.17609125905568124, 0.17609125905568124, 0.47712125471966244]\n"
     ]
    }
   ],
   "source": [
    "# CHAMP\n",
    "for k, v in docToVecChamp.items():\n",
    "    print(f\"{indToDoc[k]} : {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1,v2):\n",
    "    \"compute cosine similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)\"\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy/math.sqrt(sumxx*sumyy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 0), ('dog', 1), ('barked', 2), ('jumped', 3)]\n",
      "{'the': 1, 'dog': 1, 'barked': 1, 'jumped': 1}\n"
     ]
    }
   ],
   "source": [
    "query = \"the dog barked jumped\"\n",
    "terms = []\n",
    "line = re.sub(r'[^A-Za-z\\s]+', '', query)\n",
    "for i, word in enumerate(line.split()):\n",
    "    terms.append((word.lower(), i))\n",
    "print(terms)\n",
    "\n",
    "termToFreq = {}\n",
    "for t, p in terms:\n",
    "    termToFreq[t] = termToFreq.get(t, 0) + 1\n",
    "print(termToFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "termToId = {}\n",
    "for id, term in idToTerm.items():\n",
    "    termToId[term] = id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHAMP\n",
    "queryVectorChamp = len(idToTerm)*[float(0)]\n",
    "for term, freq in termToFreq.items():\n",
    "    if term in termToId:\n",
    "        w = 1 + math.log10(freq)\n",
    "        idf = newPostingList[termToId[term]][0]\n",
    "        queryVectorChamp[termToId[term]] = w*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHAMP\n",
    "docSimilarityChamp = []\n",
    "for doc, vec in docToVecChamp.items():\n",
    "    docSimilarityChamp.append((doc, cosine_similarity(queryVectorChamp, vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.708098191523152), (1, 0.43022968507652687), (2, 0.1017424218677466)]\n"
     ]
    }
   ],
   "source": [
    "print(docSimilarityChamp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
