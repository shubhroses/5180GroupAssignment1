{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import lucene\n",
    " \n",
    "from java.io import File\n",
    "from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
    "from org.apache.lucene import document\n",
    "from org.apache.lucene.document import Document, Field\n",
    "from org.apache.lucene.index import IndexWriter, IndexWriterConfig, IndexOptions, DirectoryReader, Term, MultiFields, Fields, FieldInfos\n",
    "from org.apache.lucene.search import IndexSearcher\n",
    "from org.apache.lucene.search.similarities import TFIDFSimilarity\n",
    "from org.apache.lucene.search.similarities import ClassicSimilarity\n",
    "from org.apache.lucene.store import FSDirectory\n",
    "from org.apache.lucene.util import Version\n",
    "from org.apache.lucene.queryparser.classic import QueryParser\n",
    "from helper import get_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INDEXER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<jcc.JCCEnv at 0x7f7cf91ddb30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lucene.initVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexPath = File(\"index/\").toPath() #from java. io import File\n",
    "indexDir = FSDirectory.open(indexPath)\n",
    "writerConfig = IndexWriterConfig(StandardAnalyzer())\n",
    "writer = IndexWriter(indexDir, writerConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docIdToText = get_docs(\"time/test.all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {1: ['The', 'dog', 'barked'], 2: ['The', 'dog', 'jumped'], 3: ['A', 'cat', 'jumped']})\n"
     ]
    }
   ],
   "source": [
    "print(docIdToText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in docIdToText.items():\n",
    "    doc = Document()\n",
    "    fieldType = document.FieldType()\n",
    "    fieldType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS)\n",
    "    fieldType.setStored(True)\n",
    "    fieldType.setTokenized(True)\n",
    "    fieldType.setOmitNorms(False)\n",
    "\n",
    "    doc.add(Field(\"myID\", k, fieldType))\n",
    "    doc.add(Field(\"content\", \" \".join(v), fieldType))\n",
    "\n",
    "    writer.addDocument(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RETRIEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "JVM is already running and updating its classpath failed. Call initVM() instead just once but with a classpath keyword argument set to the module.CLASSPATH strings of all the JCC extension modules to be imported by this process",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lucene\u001b[39m.\u001b[39;49minitVM()\n",
      "\u001b[0;31mValueError\u001b[0m: JVM is already running and updating its classpath failed. Call initVM() instead just once but with a classpath keyword argument set to the module.CLASSPATH strings of all the JCC extension modules to be imported by this process"
     ]
    }
   ],
   "source": [
    "lucene.initVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = StandardAnalyzer()\n",
    "indexPath = File(\"index\").toPath()\n",
    "indexDir = FSDirectory.open(indexPath)\n",
    "reader = DirectoryReader.open(indexDir)\n",
    "searcher = IndexSearcher(reader)\n",
    "searcher.similarity = ClassicSimilarity()\n",
    "indexReader = searcher.getIndexReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Term(\"content\", \"dog\")\n",
    "df = indexReader.docFreq(t)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(reader.numDocs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = indexReader.document(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document<stored<myID:2> stored,indexed,tokenized,indexOptions=DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS<content:The dog jumped>>\n",
      "[stored<myID:2>, stored,indexed,tokenized,indexOptions=DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS<content:The dog jumped>]\n"
     ]
    }
   ],
   "source": [
    "print(d1)\n",
    "print(d1.fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Fields' has no attribute 'getFields'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fields \u001b[39m=\u001b[39m Fields\u001b[39m.\u001b[39;49mgetFields(indexReader)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Fields' has no attribute 'getFields'"
     ]
    }
   ],
   "source": [
    "fields = Fields.getFields(indexReader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(indexReader.maxDoc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unbound method ClassicSimilarity.idfExplain() needs an argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(ClassicSimilarity\u001b[39m.\u001b[39;49midfExplain())\n",
      "\u001b[0;31mTypeError\u001b[0m: unbound method ClassicSimilarity.idfExplain() needs an argument"
     ]
    }
   ],
   "source": [
    "print(ClassicSimilarity.idfExplain())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = QueryParser(\"content\", analyzer).parse(\"a cat jumped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX = 1000\n",
    "hits = searcher.search(query, MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.698521375656128 2 doc=2 score=2.6985214 shardIndex=-1\n",
      "0.7434435486793518 1 doc=1 score=0.74344355 shardIndex=-1\n"
     ]
    }
   ],
   "source": [
    "for hit in hits.scoreDocs:\n",
    "    # print(hit)\n",
    "    print(hit.score, hit.doc, hit.toString())\n",
    "    doc = searcher.doc(hit.doc)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a document vector\n",
    "- For each document\n",
    "- Create a vector where each index is associated with a vocabulary term\n",
    "    - For each term in the document\n",
    "        - Set the value at its index in the vector to tf * idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['The', 'dog', 'barked', 'jumped', 'A', 'cat'])\n",
      "[0.17609125905568124, 0.17609125905568124, 0.47712125471966244, 0.0, 0.0, 0.0]\n",
      "[0.17609125905568124, 0.17609125905568124, 0.0, 0.17609125905568124, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.17609125905568124, 0.47712125471966244, 0.47712125471966244]\n",
      "The dog barked\n",
      "The dog jumped\n",
      "A cat jumped\n",
      "A cat barked barked barked\n",
      "[0.0, 0.0, 0.7047659464249274, 0.0, 0.47712125471966244, 0.47712125471966244]\n",
      "[(2, 0.6691470647505655), (0, 0.6403446349278143)]\n"
     ]
    }
   ],
   "source": [
    "collectionSize = reader.numDocs()\n",
    "\n",
    "# Get docTermFreqMap = {0: {'The': 1, 'dog': 1, 'barked': 1}, 1: {'The': 1, 'dog': 2, 'jumped': 1}, 2: {'A': 1, 'cat': 1, 'jumped': 1}}\n",
    "# and vocab\n",
    "vocab = {}\n",
    "docTermFreqMap = {}\n",
    "\n",
    "for i in range (collectionSize):\n",
    "    map = {}\n",
    "    curDoc = str(indexReader.document(i).get('content')).split(' ')\n",
    "    # Get TF of individual docs\n",
    "    for word in curDoc:\n",
    "        vocab[word] = True\n",
    "        if word in map:\n",
    "            map[word] = map.get(word) + 1\n",
    "        else:\n",
    "            map[word] = 1\n",
    "    docTermFreqMap[i] = map\n",
    "\n",
    "vocab = vocab.keys()\n",
    "\n",
    "#Get Doc Vectors\n",
    "docVectors = {} \n",
    "\n",
    "for i in range (collectionSize):\n",
    "    curVector = []\n",
    "\n",
    "    for word in vocab:\n",
    "        docTermFreq = docTermFreqMap.get(i).get(word)\n",
    "        weight = 0\n",
    "        if docTermFreq is not None:\n",
    "            weight = 1 + math.log10(docTermFreq)\n",
    "\n",
    "        term = Term('content', word.lower())\n",
    "        totalTermDocs = reader.docFreq(term)\n",
    "        idf = math.log10(collectionSize/totalTermDocs)\n",
    "\n",
    "        curVector.append(idf*weight)\n",
    "\n",
    "    docVectors[i] = curVector\n",
    "\n",
    "# Print out the results\n",
    "print(vocab)\n",
    "for dv in docVectors.keys():\n",
    "    print(docVectors[dv])\n",
    "\n",
    "for doc in range (collectionSize):\n",
    "    print(indexReader.document(doc).get('content'))\n",
    "\n",
    "\n",
    "# Now, To do Exact Top K, given K and a freetext Query\n",
    "k = 2\n",
    "query = 'A cat barked barked barked'\n",
    "\n",
    "# Make the query vector\n",
    "queryVector = []\n",
    "for word in vocab:\n",
    "    freq = query.count(word)\n",
    "    weight = 0\n",
    "    if freq != 0:\n",
    "        weight = 1 + math.log10(freq)\n",
    "\n",
    "    term = Term('content', word.lower())\n",
    "    totalTermDocs = reader.docFreq(term)\n",
    "    idf = math.log10(collectionSize/totalTermDocs)\n",
    "\n",
    "    queryVector.append(weight*idf)\n",
    "\n",
    "print(query)\n",
    "print(queryVector)\n",
    "\n",
    "# Now, Get Top K by ranking the results of document vectors via cosine sim\n",
    "\n",
    "cosines = []\n",
    "for i in range(collectionSize):\n",
    "    curDocVec = docVectors[i]\n",
    "    dotProduct = 0\n",
    "    magnitudeQ = 0\n",
    "    magnitudeD = 0\n",
    "    for j in range(len(vocab)):\n",
    "        dotProduct += (queryVector[j] * curDocVec[j])\n",
    "        magnitudeQ += (queryVector[j] * queryVector[j])\n",
    "        magnitudeD += (curDocVec[j] * curDocVec[j])\n",
    "    \n",
    "    cosSim = dotProduct / (math.sqrt(magnitudeQ) * math.sqrt(magnitudeD))\n",
    "    cosines.append((i, cosSim))\n",
    "\n",
    "listk = sorted(cosines, key = lambda x:x[1], reverse=True)\n",
    "print(listk[:2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document<stored<myID:1> stored,indexed,tokenized,indexOptions=DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS<content:The dog barked>>\n",
      "Document<stored<myID:2> stored,indexed,tokenized,indexOptions=DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS<content:The dog jumped>>\n",
      "Document<stored<myID:3> stored,indexed,tokenized,indexOptions=DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS<content:A cat jumped>>\n"
     ]
    }
   ],
   "source": [
    "for doc in range (indexReader.numDocs()):\n",
    "    curDoc = indexReader.document(doc)\n",
    "    curDoc = str(curDoc)\n",
    "    print(curDoc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'fields'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [100], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fields \u001b[39m=\u001b[39m reader\u001b[39m.\u001b[39;49mfields()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'fields'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
