{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import time\n",
    "import lucene\n",
    " \n",
    "from java.io import File\n",
    "from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
    "from org.apache.lucene import document\n",
    "from org.apache.lucene.document import Document, Field\n",
    "from org.apache.lucene.index import IndexWriter, IndexWriterConfig, IndexOptions, DirectoryReader, Term, MultiFields, Fields, FieldInfos\n",
    "from org.apache.lucene.search import IndexSearcher\n",
    "from org.apache.lucene.search.similarities import TFIDFSimilarity\n",
    "from org.apache.lucene.search.similarities import ClassicSimilarity\n",
    "from org.apache.lucene.store import FSDirectory\n",
    "from org.apache.lucene.util import Version\n",
    "from org.apache.lucene.queryparser.classic import QueryParser\n",
    "from helper import get_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INDEXER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<jcc.JCCEnv at 0x7f7cf91ddb30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lucene.initVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexPath = File(\"index/\").toPath() #from java. io import File\n",
    "indexDir = FSDirectory.open(indexPath)\n",
    "writerConfig = IndexWriterConfig(StandardAnalyzer())\n",
    "writer = IndexWriter(indexDir, writerConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docIdToText = get_docs(\"time/test.all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {1: ['The', 'dog', 'barked'], 2: ['The', 'dog', 'jumped'], 3: ['A', 'cat', 'jumped']})\n"
     ]
    }
   ],
   "source": [
    "print(docIdToText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in docIdToText.items():\n",
    "    doc = Document()\n",
    "    fieldType = document.FieldType()\n",
    "    fieldType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS)\n",
    "    fieldType.setStored(True)\n",
    "    fieldType.setTokenized(True)\n",
    "    fieldType.setOmitNorms(False)\n",
    "\n",
    "    doc.add(Field(\"myID\", k, fieldType))\n",
    "    doc.add(Field(\"content\", \" \".join(v), fieldType))\n",
    "\n",
    "    writer.addDocument(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RETRIEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucene.initVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = StandardAnalyzer()\n",
    "indexPath = File(\"index\").toPath()\n",
    "indexDir = FSDirectory.open(indexPath)\n",
    "reader = DirectoryReader.open(indexDir)\n",
    "searcher = IndexSearcher(reader)\n",
    "searcher.similarity = ClassicSimilarity()\n",
    "indexReader = searcher.getIndexReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(reader.numDocs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a document vector\n",
    "- For each document\n",
    "- Create a vector where each index is associated with a vocabulary term\n",
    "    - For each term in the document\n",
    "        - Set the value at its index in the vector to tf * idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab:  dict_keys(['The', 'dog', 'barked', 'jumped', 'A', 'cat'])\n",
      "DocID:  0  Vector =  [0.17609125905568124, 0.17609125905568124, 0.47712125471966244, 0.0, 0.0, 0.0]\n",
      "DocID:  1  Vector =  [0.17609125905568124, 0.17609125905568124, 0.0, 0.17609125905568124, 0.0, 0.0]\n",
      "DocID:  2  Vector =  [0.0, 0.0, 0.0, 0.17609125905568124, 0.47712125471966244, 0.47712125471966244]\n",
      "DocID:  0 The dog barked\n",
      "DocID:  1 The dog jumped\n",
      "DocID:  2 A cat jumped\n"
     ]
    }
   ],
   "source": [
    "collectionSize = reader.numDocs()\n",
    "\n",
    "# Get docTermFreqMap = {0: {'The': 1, 'dog': 1, 'barked': 1}, 1: {'The': 1, 'dog': 2, 'jumped': 1}, 2: {'A': 1, 'cat': 1, 'jumped': 1}}\n",
    "# and vocab\n",
    "vocab = {}\n",
    "docTermFreqMap = {}\n",
    "\n",
    "for i in range (collectionSize):\n",
    "    map = {}\n",
    "    curDoc = str(indexReader.document(i).get('content')).split(' ')\n",
    "    # Get TF of individual docs\n",
    "    for word in curDoc:\n",
    "        vocab[word] = True\n",
    "        if word in map:\n",
    "            map[word] = map.get(word) + 1\n",
    "        else:\n",
    "            map[word] = 1\n",
    "    docTermFreqMap[i] = map\n",
    "\n",
    "vocab = vocab.keys()\n",
    "\n",
    "#Get Doc Vectors\n",
    "docVectors = {} \n",
    "\n",
    "for i in range (collectionSize):\n",
    "    curVector = []\n",
    "\n",
    "    for word in vocab:\n",
    "        docTermFreq = docTermFreqMap.get(i).get(word)\n",
    "        weight = 0\n",
    "        if docTermFreq is not None:\n",
    "            weight = 1 + math.log10(docTermFreq)\n",
    "\n",
    "        term = Term('content', word.lower())\n",
    "        totalTermDocs = reader.docFreq(term)\n",
    "        idf = math.log10(collectionSize/totalTermDocs)\n",
    "\n",
    "        curVector.append(idf*weight)\n",
    "\n",
    "    docVectors[i] = curVector\n",
    "\n",
    "# Print out the results\n",
    "print('Vocab: ', vocab)\n",
    "for dv in docVectors.keys():\n",
    "    print('DocID: ', dv, ' Vector = ', docVectors[dv])\n",
    "\n",
    "for doc in range (collectionSize):\n",
    "    print('DocID: ', doc, indexReader.document(doc).get('content'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  cat jumped\n",
      "Query vector:  [0.0, 0.0, 0.0, 0.17609125905568124, 0.0, 0.47712125471966244]\n",
      "Results in Order of Cosine Similarity\n",
      "(2, 0.7293023054525128)\n",
      "(1, 0.19990265386264808)\n",
      "(0, 0.0)\n",
      "Time taken to query:  0.000857457\n"
     ]
    }
   ],
   "source": [
    "# Now, To do Exact Top K, given K and a freetext Query\n",
    "k = int(input(\"Enter K:\"))\n",
    "if k > collectionSize:\n",
    "    k = collectionSize\n",
    "query = input(\"Enter Query:\")\n",
    "timeStart = time.time_ns()\n",
    "\n",
    "# Make the query vector\n",
    "queryVector = []\n",
    "for word in vocab:\n",
    "    freq = query.count(word)\n",
    "    weight = 0\n",
    "    if freq != 0:\n",
    "        weight = 1 + math.log10(freq)\n",
    "\n",
    "    term = Term('content', word.lower())\n",
    "    totalTermDocs = reader.docFreq(term)\n",
    "    idf = math.log10(collectionSize/totalTermDocs)\n",
    "\n",
    "    queryVector.append(weight*idf)\n",
    "\n",
    "print('Query: ', query)\n",
    "print('Query vector: ',queryVector)\n",
    "\n",
    "# Now, Get Top K by ranking the results of document vectors via cosine sim\n",
    "\n",
    "cosines = []\n",
    "for i in range(collectionSize):\n",
    "    curDocVec = docVectors[i]\n",
    "    magnitudeD, magnitudeQ, dotProduct = 0, 0, 0\n",
    "    for j in range(len(vocab)):\n",
    "        dotProduct += (queryVector[j] * curDocVec[j])\n",
    "        magnitudeQ += (queryVector[j] * queryVector[j])\n",
    "        magnitudeD += (curDocVec[j] * curDocVec[j])\n",
    "    \n",
    "    cosSim = dotProduct / (math.sqrt(magnitudeQ) * math.sqrt(magnitudeD))\n",
    "    cosines.append((i, cosSim))\n",
    "\n",
    "listk = sorted(cosines, key = lambda x:x[1], reverse=True)\n",
    "print ('Results in Order of Cosine Similarity')\n",
    "for i in range(k):\n",
    "    print(listk[i])\n",
    "\n",
    "timeEnd = time.time_ns()\n",
    "\n",
    "print('Time taken to query: ', (timeEnd-timeStart)/(10**9))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
