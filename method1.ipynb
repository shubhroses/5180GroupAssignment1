{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import os\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "indToDoc = {}\n",
    "postingList = collections.defaultdict(list)\n",
    "curDocID = 0\n",
    "path = \"practice_collection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(f, path):\n",
    "    #Returns [(term, pos), (term, pos) ...]\n",
    "    terms = []\n",
    "    with open(path + \"/\" + f) as file:\n",
    "        line = file.readline()\n",
    "        i = 0\n",
    "        while line:\n",
    "            # Regex to match only strings and spaces \n",
    "            line = re.sub(r'[^A-Za-z\\s]+', '', line)\n",
    "            for word in line.split():\n",
    "                terms.append((word.lower(), i))\n",
    "                i += 1\n",
    "            line = file.readline() \n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createOldPostingList(path):\n",
    "    startTime = time.time()\n",
    "\n",
    "    stopList = set()\n",
    "    stopTuples = tokenize(\"stop-list.txt\", \".\")\n",
    "    for t, p in stopTuples:\n",
    "        stopList.add(t)\n",
    "\n",
    "    for f in os.listdir(path):\n",
    "        # TODO: remove in real code\n",
    "        global curDocID\n",
    "        \n",
    "        indToDoc[curDocID] = f\n",
    "\n",
    "        # Returns [(word, pos), (word, pos) ...]\n",
    "        terms = tokenize(f, path)\n",
    "\n",
    "        # create map {word:[pos1, pos2, pos3]}\n",
    "        wordToPos = collections.defaultdict(list)\n",
    "        for word, pos in terms:\n",
    "            # if word in stopList: TODO remove in real code\n",
    "            #     continue\n",
    "            wordToPos[word].append(pos)\n",
    "\n",
    "        # append to posting list {term : [(docID1, [pos1, pos2, pos3, pos4])]}\n",
    "        for term, arr in wordToPos.items():\n",
    "            postingList[term].append((curDocID, wordToPos[term]))\n",
    "\n",
    "        # For every file update id \n",
    "        curDocID += 1\n",
    "    \n",
    "    endTime = time.time()\n",
    "    print(f\"Index built in {endTime - startTime} seconds.\")\n",
    "    return postingList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new posting list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "idToTerm = {}\n",
    "curTermIndex = 0\n",
    "termToId = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createNewPostringList():\n",
    "    global curTermIndex # TODO REMOVE\n",
    "    newPostingList = collections.defaultdict(list)\n",
    "    result = createOldPostingList(path)\n",
    "    for k,v in result.items():\n",
    "        # if k in stopList:\n",
    "        #     continue\n",
    "        idToTerm[curTermIndex] = k\n",
    "        newPostingList[curTermIndex].append(math.log10(len(indToDoc)/len(v)))\n",
    "        \n",
    "        for docId, posList in v:\n",
    "            newPostingList[curTermIndex].append((docId, 1 + math.log10(len(posList)), posList))\n",
    "        curTermIndex += 1\n",
    "    for id, term in idToTerm.items():\n",
    "        termToId[term] = id\n",
    "    return newPostingList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index built in 0.0026581287384033203 seconds.\n"
     ]
    }
   ],
   "source": [
    "newPostingList = createNewPostringList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : [0.17609125905568124, (0, 1.0, [0]), (1, 1.0, [0])]\n",
      "1 : [0.17609125905568124, (0, 1.0, [1]), (1, 1.0, [1])]\n",
      "2 : [0.47712125471966244, (0, 1.0, [2])]\n",
      "3 : [0.17609125905568124, (1, 1.0, [2]), (2, 1.0, [2])]\n",
      "4 : [0.47712125471966244, (2, 1.0, [0])]\n",
      "5 : [0.47712125471966244, (2, 1.0, [1])]\n",
      "{0: 'the', 1: 'dog', 2: 'barked', 3: 'jumped', 4: 'a', 5: 'cat'}\n"
     ]
    }
   ],
   "source": [
    "for k, v in newPostingList.items():\n",
    "    print(f\"{k} : {v}\")\n",
    "print(idToTerm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertDocumentsToVector():\n",
    "    docToVec = {}\n",
    "    for k in indToDoc.keys():\n",
    "        docToVec[k] = len(idToTerm)*[float(0)]\n",
    "\n",
    "    for k, v in newPostingList.items():\n",
    "        idf = v[0]\n",
    "        for i in range(1, len(v)):\n",
    "            t = v[i]\n",
    "            doc, w = t[0], t[1]\n",
    "            docToVec[doc][k] = idf*w\n",
    "    return docToVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1,v2):\n",
    "    \"compute cosine similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)\"\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy/math.sqrt(sumxx*sumyy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37780020399389935\n"
     ]
    }
   ],
   "source": [
    "docToVec = convertDocumentsToVector()\n",
    "print(cosine_similarity(docToVec[0], docToVec[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'the', 1: 'dog', 2: 'barked', 3: 'jumped', 4: 'a', 5: 'cat'}\n",
      "0: [0.17609125905568124, 0.17609125905568124, 0.47712125471966244, 0.0, 0.0, 0.0]\n",
      "1: [0.17609125905568124, 0.17609125905568124, 0.0, 0.17609125905568124, 0.0, 0.0]\n",
      "2: [0.0, 0.0, 0.0, 0.17609125905568124, 0.47712125471966244, 0.47712125471966244]\n"
     ]
    }
   ],
   "source": [
    "print(idToTerm)\n",
    "for k, v in docToVec.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_query(query_terms, k):\n",
    "    query_terms = [word.lower() for word in query_terms]\n",
    "\n",
    "    termToFreq = {}\n",
    "    for t in query_terms:\n",
    "        termToFreq[t] = termToFreq.get(t, 0) + 1\n",
    "\n",
    "    queryVector = len(idToTerm)*[float(0)]\n",
    "    for term, freq in termToFreq.items():\n",
    "        if term in termToId:\n",
    "            w = 1 + math.log10(freq)\n",
    "            idf = newPostingList[termToId[term]][0]\n",
    "            queryVector[termToId[term]] = w*idf\n",
    "\n",
    "    docToSimilarity = []\n",
    "    for doc, vec in docToVec.items():\n",
    "        docToSimilarity.append((doc, cosine_similarity(queryVector, vec)))\n",
    "    for d, s in docToSimilarity:\n",
    "        print(f\"doc: {indToDoc[d]}, similarity: {s}\")\n",
    "    docToSimilarity.sort(key = lambda x:x[1], reverse=True)\n",
    "    return [indToDoc[t[0]] for t in docToSimilarity[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: d1.txt, similarity: 0.0\n",
      "doc: d2.txt, similarity: 0.14578946632810494\n",
      "doc: d3.txt, similarity: 1.0\n",
      "['d3.txt', 'd2.txt']\n"
     ]
    }
   ],
   "source": [
    "query_terms = [\"a\", \"cat\", \"jumped\"]\n",
    "print(exact_query(query_terms, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1.0, [0]), (1, 1.0, [0])]\n",
      "[(0, 1.0, [1]), (1, 1.0, [1])]\n",
      "[(0, 1.0, [2])]\n",
      "[(1, 1.0, [2]), (2, 1.0, [2])]\n",
      "[(2, 1.0, [0])]\n",
      "[(2, 1.0, [1])]\n"
     ]
    }
   ],
   "source": [
    "championList = collections.defaultdict(list)\n",
    "r = 2\n",
    "for termId, postList in newPostingList.items():\n",
    "    idf = postList[0]\n",
    "    documents = postList[1:]\n",
    "    documents.sort(key=lambda x:x[1])\n",
    "    print(documents)\n",
    "    championList[termId].append(idf)\n",
    "    championList[termId].extend(documents[:r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {0: [0.17609125905568124, (0, 1.0, [0]), (1, 1.0, [0])], 1: [0.17609125905568124, (0, 1.0, [1]), (1, 1.0, [1])], 2: [0.47712125471966244, (0, 1.0, [2])], 3: [0.17609125905568124, (1, 1.0, [2]), (2, 1.0, [2])], 4: [0.47712125471966244, (2, 1.0, [0])], 5: [0.47712125471966244, (2, 1.0, [1])]})\n"
     ]
    }
   ],
   "source": [
    "print(championList)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
