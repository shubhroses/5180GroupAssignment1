{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Old Postring List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "indToDoc = {}\n",
    "postingList = collections.defaultdict(list)\n",
    "curDocID = 0\n",
    "path = \"practice3_collection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(f, path):\n",
    "    #Returns [(term, pos), (term, pos) ...]\n",
    "    terms = []\n",
    "    with open(path + \"/\" + f) as file:\n",
    "        line = file.readline()\n",
    "        i = 0\n",
    "        while line:\n",
    "            # Regex to match only strings and spaces \n",
    "            line = re.sub(r'[^A-Za-z\\s]+', '', line)\n",
    "            for word in line.split():\n",
    "                terms.append((word.lower(), i))\n",
    "                i += 1\n",
    "            line = file.readline() \n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPostingList(path):\n",
    "    startTime = time.time()\n",
    "    for f in os.listdir(path):\n",
    "        # TODO: remove in real code\n",
    "        global curDocID\n",
    "        \n",
    "        indToDoc[curDocID] = f\n",
    "\n",
    "        # Returns [(word, pos), (word, pos) ...]\n",
    "        terms = tokenize(f, path)\n",
    "\n",
    "        # create map {word:[pos1, pos2, pos3]}\n",
    "        wordToPos = collections.defaultdict(list)\n",
    "        for word, pos in terms:\n",
    "            wordToPos[word].append(pos)\n",
    "\n",
    "        # append to posting list {term : [(docID1, [pos1, pos2, pos3, pos4])]}\n",
    "        for term, arr in wordToPos.items():\n",
    "            postingList[term].append((curDocID, wordToPos[term]))\n",
    "\n",
    "        # For every file update id \n",
    "        curDocID += 1\n",
    "    \n",
    "    endTime = time.time()\n",
    "    print(f\"Index built in {endTime - startTime} seconds.\")\n",
    "    return postingList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index built in 0.0010848045349121094 seconds.\n"
     ]
    }
   ],
   "source": [
    "result = createPostingList(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create New Postring List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPostingList = collections.defaultdict(list)\n",
    "idToTerm = {}\n",
    "curTermIndex = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'it', 'that', 'its', 'will', 'an', 'and', 'from', 'with', 'were', 'the', 'for', 'has', 'he', 'by', 'a', 'as', 'is', 'to', 'be', 'of', 'was', 'at', 'on', 'are', 'in'}\n"
     ]
    }
   ],
   "source": [
    "stopList = set()\n",
    "stopTuples = tokenize(\"stop-list.txt\", \".\")\n",
    "for t, p in stopTuples:\n",
    "    stopList.add(t)\n",
    "print(stopList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in result.items():\n",
    "    # if k in stopList:\n",
    "    #     continue\n",
    "    idToTerm[curTermIndex] = k\n",
    "    newPostingList[curTermIndex].append(math.log10(len(indToDoc)/len(v)))\n",
    "    \n",
    "    for docId, posList in v:\n",
    "        newPostingList[curTermIndex].append((docId, 1 + math.log10(len(posList)), posList))\n",
    "    curTermIndex += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {0: [0.17609125905568124, (0, 1.3010299956639813, [0, 4]), (1, 1.3010299956639813, [0, 4])], 1: [0.17609125905568124, (0, 1.3010299956639813, [1, 6]), (1, 1.0, [1])], 2: [0.47712125471966244, (0, 1.0, [2])], 3: [0.17609125905568124, (0, 1.0, [3]), (1, 1.0, [3])], 4: [0.47712125471966244, (0, 1.0, [5])], 5: [0.17609125905568124, (1, 1.0, [2]), (2, 1.0, [2])], 6: [0.17609125905568124, (1, 1.0, [5]), (2, 1.0, [1])], 7: [0.47712125471966244, (2, 1.0, [0])]})\n"
     ]
    }
   ],
   "source": [
    "print(newPostingList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Each Document To A Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "docToVec = {}\n",
    "for k in indToDoc.keys():\n",
    "    docToVec[k] = len(idToTerm)*[float(0)]\n",
    "\n",
    "for k, v in newPostingList.items():\n",
    "    idf = v[0]\n",
    "    for i in range(1, len(v)):\n",
    "        t = v[i]\n",
    "        doc, w = t[0], t[1]\n",
    "        docToVec[doc][k] = idf*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1,v2):\n",
    "    \"compute cosine similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)\"\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    if sumxx == 0 or sumyy == 0:\n",
    "        return 0\n",
    "    return sumxy/math.sqrt(sumxx*sumyy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign leaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leadersArr = random.sample(range(0, len(indToDoc)), int(math.sqrt(len(indToDoc))))\n",
    "leadersSet = set(leadersArr)\n",
    "docToNearestLeader = {}\n",
    "leaderAndFollowers = collections.defaultdict(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each document find nearest leader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in range(len(indToDoc)):\n",
    "    docVec = docToVec[doc]\n",
    "    distNearest = float(\"-inf\")\n",
    "    nearest = None\n",
    "    for l in leadersArr:\n",
    "        newDist = cosine_similarity(docVec, docToVec[l])\n",
    "        if newDist >= distNearest:\n",
    "            distNearest = newDist\n",
    "            nearest = l\n",
    "    docToNearestLeader[doc] = nearest\n",
    "\n",
    "for k, v in docToNearestLeader.items():\n",
    "    leaderAndFollowers[v].append(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Query Into Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_terms = [\"a\", \"cat\", \"jumped\"]\n",
    "\n",
    "termToId = {}\n",
    "for id, term in idToTerm.items():\n",
    "    termToId[term] = id\n",
    "    \n",
    "query_terms = [word.lower() for word in query_terms]\n",
    "termToFreq = {}\n",
    "for t in query_terms:\n",
    "    termToFreq[t] = termToFreq.get(t, 0) + 1\n",
    "\n",
    "queryVector = len(idToTerm)*[float(0)]\n",
    "for term, freq in termToFreq.items():\n",
    "    if term in termToId:\n",
    "        w = 1 + math.log10(freq)\n",
    "        idf = newPostingList[termToId[term]][0]\n",
    "        queryVector[termToId[term]] = w*idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find distance of query to leaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "leadersAndDist = []\n",
    "for l in leadersArr:\n",
    "    v = docToVec[l]\n",
    "    distToQuery = cosine_similarity(v, queryVector)\n",
    "    leadersAndDist.append((l, distToQuery))\n",
    "leadersAndDist.sort(key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.2742612593365751)]\n"
     ]
    }
   ],
   "source": [
    "print(leadersAndDist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the cluster around nearest leader, sort all docs in cluster by distance to query vector and append them to result. If you run out of documents in current cluster, move on to next closest leader and that cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for l, dist in leadersAndDist:\n",
    "    cluster = leaderAndFollowers[l]\n",
    "    # Sort each element in cluster by distance to query vector\n",
    "    clusterDistToQuery = []\n",
    "    for n in cluster:\n",
    "        dist = cosine_similarity(docToVec[n], queryVector)\n",
    "        clusterDistToQuery.append((n, dist))\n",
    "    clusterDistToQuery.sort(key=lambda x:x[1])\n",
    "    res.extend([x[0] for x in clusterDistToQuery])\n",
    "    if len(res) >= k:\n",
    "        break\n",
    "result = res[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
